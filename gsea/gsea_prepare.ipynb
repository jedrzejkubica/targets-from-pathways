{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bfb14f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "import os\n",
    "import shutil\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a030b9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"gsea-prepare\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1567fbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"/Users/polina/targets-from-pathways/data/association_by_datatype_indirect\"\n",
    "output_dir = \"/Users/polina/targets-from-pathways/data/input/auto-input\"\n",
    "tmp_dir = os.path.join(output_dir, \"_spark_tmp_export\")\n",
    "\n",
    "# Read parquet dataset\n",
    "associations_df = spark.read.parquet(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d9d9caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+----------+--------------------+-------------+\n",
      "|  diseaseId|       targetId|datatypeId|               score|evidenceCount|\n",
      "+-----------+---------------+----------+--------------------+-------------+\n",
      "|EFO_0008625|ENSG00000112715|literature|0.030396539880581056|            1|\n",
      "|EFO_0008626|ENSG00000004478|literature|0.030396539880581056|            1|\n",
      "|EFO_0008626|ENSG00000007171|literature|0.012158615952232422|            1|\n",
      "|EFO_0008626|ENSG00000010610|literature| 0.03951550184475537|            2|\n",
      "|EFO_0008626|ENSG00000019582|literature|0.012158615952232422|            1|\n",
      "+-----------+---------------+----------+--------------------+-------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "associations_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39ad3721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote TSV: /Users/polina/targets-from-pathways/data/input/auto-input/final_gsea_input.tsv\n"
     ]
    }
   ],
   "source": [
    "# Build final gene-level TSV: map targetId->approvedSymbol, group, rename, write\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "output_dir = \"/Users/polina/targets-from-pathways/data/input/auto-input\"\n",
    "input_associations_path = os.path.join(output_dir, \"filtered_associations.tsv\")\n",
    "targets_path = \"/Users/polina/targets-from-pathways/data/gene_data.txt\"\n",
    "tmp_dir = os.path.join(output_dir, \"_spark_tmp_export_final\")\n",
    "\n",
    "spark = globals().get(\"spark\")\n",
    "if spark is None:\n",
    "    spark = SparkSession.builder.appName(\"gsea-prepare\").getOrCreate()\n",
    "    globals()[\"spark\"] = spark\n",
    "\n",
    "# Read previously filtered associations\n",
    "assoc_df = spark.read.csv(input_associations_path, sep=\"\\t\", header=True, inferSchema=True)\n",
    "\n",
    "# Read target (gene) mapping and select only id and approvedSymbol columns\n",
    "# gene_id corresponds to Ensembl id; gene_name is the approved gene symbol\n",
    "targets_df = (\n",
    "    spark.read.csv(targets_path, sep=\"\\t\", header=True, inferSchema=True)\n",
    "    .select(F.col(\"gene_id\").alias(\"id\"), F.col(\"gene_name\").alias(\"approvedSymbol\"))\n",
    ")\n",
    "\n",
    "# Join associations with targets on targetId == id\n",
    "joined_df = assoc_df.join(targets_df, assoc_df.targetId == targets_df.id, \"inner\")\n",
    "\n",
    "# Ensure score is numeric before aggregation\n",
    "joined_df = joined_df.withColumn(\"score\", F.col(\"score\").cast(\"double\"))\n",
    "\n",
    "# Aggregate by approvedSymbol, take max score per gene, and rename columns\n",
    "final_df = (\n",
    "    joined_df.groupBy(\"approvedSymbol\").agg(F.max(\"score\").alias(\"globalScore\"))\n",
    "    .select(F.col(\"approvedSymbol\").alias(\"symbol\"), F.col(\"globalScore\"))\n",
    "    .orderBy(F.desc(\"globalScore\"))\n",
    ")\n",
    "\n",
    "# Write as a single TSV\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "if os.path.exists(tmp_dir):\n",
    "    shutil.rmtree(tmp_dir)\n",
    "\n",
    "final_df.coalesce(1).write.mode(\"overwrite\").option(\"sep\", \"\\t\").option(\"header\", True).csv(tmp_dir)\n",
    "\n",
    "part_files = glob.glob(os.path.join(tmp_dir, \"part-*\"))\n",
    "if not part_files:\n",
    "    raise RuntimeError(\"No part files were written by Spark; export failed.\")\n",
    "\n",
    "final_tsv_path = os.path.join(output_dir, \"final_gsea_input.tsv\")\n",
    "if os.path.exists(final_tsv_path):\n",
    "    os.remove(final_tsv_path)\n",
    "\n",
    "shutil.move(part_files[0], final_tsv_path)\n",
    "shutil.rmtree(tmp_dir)\n",
    "\n",
    "print(f\"Wrote TSV: {final_tsv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49b43d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote TSV: /Users/polina/targets-from-pathways/data/input/auto-input/final_gsea_input.tsv\n"
     ]
    }
   ],
   "source": [
    "# Build final gene-level TSV from target parquet: map id->approvedSymbol, group, rename, write\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "output_dir = \"/Users/polina/targets-from-pathways/data/input/auto-input\"\n",
    "input_associations_path = os.path.join(output_dir, \"filtered_associations.tsv\")\n",
    "targets_path = \"/Users/polina/targets-from-pathways/data/target\"\n",
    "tmp_dir = os.path.join(output_dir, \"_spark_tmp_export_final\")\n",
    "\n",
    "spark = globals().get(\"spark\")\n",
    "if spark is None:\n",
    "    spark = SparkSession.builder.appName(\"gsea-prepare\").getOrCreate()\n",
    "    globals()[\"spark\"] = spark\n",
    "\n",
    "# Read previously filtered associations\n",
    "assoc_df = spark.read.csv(input_associations_path, sep=\"\\t\", header=True, inferSchema=True)\n",
    "\n",
    "# Read target parquet and auto-detect id and approved symbol columns\n",
    "raw_targets_df = spark.read.parquet(targets_path)\n",
    "fields_lower_to_actual = {f.name.lower(): f.name for f in raw_targets_df.schema.fields}\n",
    "\n",
    "id_candidates = [\"id\", \"targetid\", \"gene_id\", \"ensembl_id\"]\n",
    "symbol_candidates = [\"approvedsymbol\", \"gene_name\", \"approved_symbol\", \"symbol\"]\n",
    "\n",
    "id_col_actual = next((fields_lower_to_actual[c] for c in id_candidates if c in fields_lower_to_actual), None)\n",
    "symbol_col_actual = next((fields_lower_to_actual[c] for c in symbol_candidates if c in fields_lower_to_actual), None)\n",
    "\n",
    "if id_col_actual is None or symbol_col_actual is None:\n",
    "    available = list(fields_lower_to_actual.values())\n",
    "    raise RuntimeError(f\"Could not find required columns in target parquet. Available: {available}\")\n",
    "\n",
    "targets_df = raw_targets_df.select(\n",
    "    F.col(id_col_actual).alias(\"id\"),\n",
    "    F.col(symbol_col_actual).alias(\"approvedSymbol\"),\n",
    ")\n",
    "\n",
    "# Join associations with targets on targetId == id\n",
    "joined_df = assoc_df.join(targets_df, assoc_df.targetId == targets_df.id, \"inner\")\n",
    "\n",
    "# Ensure score is numeric before aggregation\n",
    "joined_df = joined_df.withColumn(\"score\", F.col(\"score\").cast(\"double\"))\n",
    "\n",
    "# Aggregate by approvedSymbol, take max score per gene, and rename columns\n",
    "final_df = (\n",
    "    joined_df.groupBy(\"approvedSymbol\").agg(F.max(\"score\").alias(\"globalScore\"))\n",
    "    .select(F.col(\"approvedSymbol\").alias(\"symbol\"), F.col(\"globalScore\"))\n",
    "    .orderBy(F.desc(\"globalScore\"))\n",
    ")\n",
    "\n",
    "# Write as a single TSV\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "if os.path.exists(tmp_dir):\n",
    "    shutil.rmtree(tmp_dir)\n",
    "\n",
    "final_df.coalesce(1).write.mode(\"overwrite\").option(\"sep\", \"\\t\").option(\"header\", True).csv(tmp_dir)\n",
    "\n",
    "part_files = glob.glob(os.path.join(tmp_dir, \"part-*\"))\n",
    "if not part_files:\n",
    "    raise RuntimeError(\"No part files were written by Spark; export failed.\")\n",
    "\n",
    "final_tsv_path = os.path.join(output_dir, \"final_gsea_input.tsv\")\n",
    "if os.path.exists(final_tsv_path):\n",
    "    os.remove(final_tsv_path)\n",
    "\n",
    "shutil.move(part_files[0], final_tsv_path)\n",
    "shutil.rmtree(tmp_dir)\n",
    "\n",
    "print(f\"Wrote TSV: {final_tsv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9615da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
